---
output: pdf_document
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{How-to-use-the-EUFLOW-package}
-->


---
title: "EUFLOW: Expected-Utility-based Workflow Evaluation"
author: "Kevin K. McDade"
date: "`r Sys.Date()`"
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteKeyword{EUFLOW}
  %\SweaveUTF8
  %\usepackage[utf8](inputenc)
output: 
    rmarkdown::html_vignette:
        toc: yes
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
library("EUFLOW")
options(stringsAsFactors = FALSE)
options(digits=3)
system("touch I_ran_Rmd")
```
### I. Purpose of the EUFLOW package

The data in bioinformatics is often in some “raw” form which is not yet ready for analysis. Processing this data often involves several steps, called variously a workflow, pipeline, or protocol. We have developed the EUFLOW package to assist users who have multiple versions of data that arise from different *workflow paths*. Each of the *final processed data* files should contain similar features as the rows of the dataframe with columns of identical samples. The collective *workflow path data* is refered to as the *Evaluation Dataset*.  In order to evaluate the alternitive versions of the data the user must also provide a *Reference Dataset*. The Evaluation Dataset and the Reference Dataset must have an expected quantitative relationship, such as mRNA/protein coorelation. The EUFLOW package utilizes this relationship (*Model Quality*) through our methodology to determine the *Expected Utility* for each of the *workflow paths*.  

### II. Usage of EUFLOW

Users are required to input two separate data files, regardless of the analysis type. The EUFLOW package depends upon a model quality heuristic which we will demonstate here as correlation between gene expression and protein expression. The first data file, for example, we will use ovarian TCGA RNASeq data on two separate workflows provided on the same samples. Only samples for which protein expression data was available are used in the RNASEQDATA file, the HUGO Gene names and descriptions for these proteins are presented in the table below.



A small slice of this data file (RNASEQDATA) demonstrates the data structure of numbered row names followed by a column of identifiers including a tag _v1 and _v2. The same samples were processed with two different workflow options RnaSeqv1 and RnaSeqv2. The output below shows the gene expression values for the first 9 identifiers (in this case gene names) with a tag to distinguish workflow option. For brevity only 4 of the 198 sample identifiers are represented.


```{r}
data(RNASEQDATA)
RNASEQDATA[1:9,1:3]
```


In the second data file we will use TCGA protein expression data on the samples. The same 198 samples are represented in the same order. As above we will output only a subset. Alos note that as this is the reference dataset in this example only one identifier is represented rather than multiple workflow options.   
```{r}
data(RPPADATA.original)
RPPADATA<-RPPADATA.original
RPPADATA[1:9,1:3]

```
We now have all that is required to run the EUFLOW package. For the purpose of clarity we define the RNASEQDATA set as the EvaluationExperimentSet, which includes 2 workflow options (RnaSeqV1 and RnaSeqv2). We also define the RPPADATA as the ReferenceSet. If it is the choice of the user multiple reference sets can be utilized. 

```{r}
EvaluationExperimentSet<-RNASEQDATA
ReferenceSet<-RPPADATA
```

Now that we have the data for our example, the WorkflowPathData function will modify the separate dataframes into one data structure to prepare to calcuate the model quality and perform the evaluation. The first item in the list is the reference data and the second item in the list is the evaluation data.

```{r,eval=TRUE}
Workflow.Path.Data<-WorkflowPathData(EvaluationExperimentSet,ReferenceSet)
Workflow.Path.Data[[1]][1:9,1:3]
```

Labels are assigned using the BuildEvaluationStructure function to create a data structure that can sort by reference identifiers and evaluation identifiers. The tags are _reference for the reference data and _path for the evaluation data. In addition, the evaluation data tags have a workflow label to specify the type of evaluation data. In this example two evaluation workflow paths are assigned _RNASeqv1 amd _RNASeqv2. 

```{r}
Evaluation.Structure<-BuildEvaluationStructure(Workflow.Path.Data,EvaluationTag=c("RNASeqv1","RNASEQv2"))
Evaluation.Structure[1:9,1:3]
```
The function WorkflowPathModelQuality creates a map between the reference ids and the evaluation ids. The Path.Model.Quality object contains all of the pairs across the two platforms.

```{r}
Path.Model.Quality<-WorkflowPathModelQuality(Evaluation.Structure)

```
Next, Using the function ModelQualityPairs on the object Path.Model.Quality the user can determine the appropriate model quality for this evaluation. Model.Quality.Values is an object which contains the model quality values for each of the pairs. How the values are determined is specified by the user. In this example Pearson correlation r values are calculated for each Reference-Evaluation pair across all samples.

```{r}
Model.Quality.Values<-ModelQualityPairs(Path.Model.Quality,method="pearson")
head(as.data.frame(Model.Quality.Values))
```

Other model quality values can be calculated using the "method" argument in the  function. Sperman r values are calculated in this example.

```{r}
Model.Quality.Values<-ModelQualityPairs(Path.Model.Quality,method="spearman")
head(as.data.frame(Model.Quality.Values))
```
Next the correlation values and the reference-evaluation pairs are are the input to the EstimatePosteriorProbability function. The first step of this function is to apply a bootstrapping procedure to obtain a resampled standard deviation and bias of the model quality values. Next the vector of correlation r values, the variance, and the bias are the input to the EM procedure to estimate the posterior probability and posterior probability variance of belonging to the "+" component. In the figure below a mixture distribution is estmated that has 2 components where one component represents the "+" component and the 0 centered component represents the "-" and "0" component.   

```{r,message=FALSE}
Posterior.Probability<-EstimatePosteriorProbability(Path.Model.Quality,Model.Quality.Values)
head(Posterior.Probability)
```

A user can now input values for the *Utility of a true positive Utp*, *Loss of a false positive Lfp*, a *deltaPlus* value, and a *guarantee* threshold for the estimation of the *Expected Utility* of each method. Evaluation.table will take the input value of Posterior.dataframe to calculate the following values: 
See Chapter 3 for a full description of the calculation of these values. The default values of Utp=1,Lfp=1, deltaPlus =1, and guarantee=1e-5 are used unless the user enters a new value.

```{r}
Evaluation.table<-WorkflowEvaluationTable(Posterior.Probability)
Evaluation.table

```
A change of the deltaPlus parameter changes the proportional relationship between PrPlus and PrTrue. and Changing the Utlity of a True positive places more user utility on the true positive value as opposed to the false positive. Table X below has a value of deltaPlus = 2. Table X has a Utp = 3.

```{r}
WorkflowEvaluationTable(Posterior.Probability,deltaPlus = 2)
WorkflowEvaluationTable(Posterior.Probability,Utp=3)

```


### III. Examples of EUFLOW Experiments


In each of the following examples the EUFLOW package will be used to evaluate a workflow path decision on real data. The first example is an RNASeq experiment on ovarian samples across three different workflow paths, RNASeqV1, RNASeqV2, and the PICCOLO workflow path. 

```{r}
data(RNASEQV1v2PICCOLO)
EvaluationExperimentSet<-RNASEQV1v2PICCOLO
dim(RNASEQV1v2PICCOLO)
ReferenceSet<-RPPADATA
dim(ReferenceSet)
Workflow.Path.Data<-WorkflowPathData(EvaluationExperimentSet,ReferenceSet)
Evaluation.Structure<-BuildEvaluationStructure(Workflow.Path.Data,EvaluationTag=c("RNASeqv1","RNASeqv2","PICCOLO"))
Path.Model.Quality<-WorkflowPathModelQuality(Evaluation.Structure)
Model.Quality.Values<-ModelQualityPairs(Path.Model.Quality,method="pearson")
Posterior.Probability<-EstimatePosteriorProbability(Path.Model.Quality,Model.Quality)
Evaluation.table<-WorkflowEvaluationTable(Posterior.Probability)
Evaluation.table

Evaluation.tableL2<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 2)
Evaluation.tableL3<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 3)
Evaluation.tableL4<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 4)
Evaluation.tableL5<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 5)
Evaluation.tableL6<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 6)
LossPlot<-data.frame(Evaluation.table$Eutility,Evaluation.tableL2$Eutility,Evaluation.tableL3$Eutility,Evaluation.tableL4$Eutility,Evaluation.tableL5$Eutility,Evaluation.tableL6$Eutility)
#names(UtilityPlot)<-c("LFP1","LFP2","LFP3","LFP4","LFP5","LFP6")
plot(1:6,LossPlot[1,],ylim = c(0,200),type = "l",col="blue",main="LFP vs Total Expected Utility",xlab="LFP",ylab="TEU")
lines(1:6,LossPlot[2,],col="red")
lines(1:6,LossPlot[3,],col="green")
lines(1:6,LossPlot[4,],col="orange")
lines(1:6,LossPlot[5,])

Evaluation.tableU2<-WorkflowEvaluationTable(Posterior.Probability,Utp = 2)
Evaluation.tableU3<-WorkflowEvaluationTable(Posterior.Probability,Utp = 3)
Evaluation.tableU4<-WorkflowEvaluationTable(Posterior.Probability,Utp = 4)
Evaluation.tableU5<-WorkflowEvaluationTable(Posterior.Probability,Utp = 5)
Evaluation.tableU6<-WorkflowEvaluationTable(Posterior.Probability,Utp = 6)
UtilityPlot<-data.frame(Evaluation.table$Eutility,Evaluation.tableU2$Eutility,Evaluation.tableU3$Eutility,Evaluation.tableU4$Eutility,Evaluation.tableU5$Eutility,Evaluation.tableU6$Eutility)
#names(UtilityPlot)<-c("LFP1","LFP2","LFP3","LFP4","LFP5","LFP6")
plot(1:6,UtilityPlot[1,],ylim = c(0,200),type = "l",col="blue",main="UTP vs Total Expected Utility",xlab="UTP",ylab="TEU")
lines(1:6,UtilityPlot[2,],col="red")
lines(1:6,UtilityPlot[3,],col="green")
lines(1:6,UtilityPlot[4,],col="orange")
lines(1:6,UtilityPlot[5,])


```
The second experiment uses the BRCA-TCGA data.

```{r}
data(BRCARPPADATAFINAL)
dim(BRCARPPADATAFINAL)
data(BRCASALMONDATAFINAL)
dim(BRCASALMONDATAFINAL)
EvaluationExperimentSet<-BRCASALMONDATAFINAL
ReferenceSet<-BRCARPPADATAFINAL
Workflow.Path.Data<-WorkflowPathData(EvaluationExperimentSet,ReferenceSet)
Evaluation.Structure<-BuildEvaluationStructure(Workflow.Path.Data,EvaluationTag=c("AllGeneIDs","TransmembraneGenesOnly","Non-TransmembraneGenes","Low-Complexity","High-Complexity"))
Path.Model.Quality<-WorkflowPathModelQuality(Evaluation.Structure)
Model.Quality.Values<-ModelQualityPairs(Path.Model.Quality,method="pearson")
as.data.frame(Model.Quality.Values)
Posterior.Probability<-EstimatePosteriorProbability(Path.Model.Quality,Model.Quality.Values)
data.frame(as.data.frame(Model.Quality.Values)$workflow_paths_combined,Posterior.Probability)
Evaluation.table<-WorkflowEvaluationTable(Posterior.Probability)
Evaluation.table
Evaluation.tableL2<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 2)
Evaluation.tableL3<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 3)
Evaluation.tableL4<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 4)
Evaluation.tableL5<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 5)
Evaluation.tableL6<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 6)
LossPlot<-data.frame(Evaluation.table$Eutility,Evaluation.tableL2$Eutility,Evaluation.tableL3$Eutility,Evaluation.tableL4$Eutility,Evaluation.tableL5$Eutility,Evaluation.tableL6$Eutility)
#names(UtilityPlot)<-c("LFP1","LFP2","LFP3","LFP4","LFP5","LFP6")
plot(1:6,LossPlot[1,],ylim = c(0,50),type = "l",col="blue",main="LFP vs Total Expected Utility",xlab="LFP",ylab="TEU")
lines(1:6,LossPlot[2,],col="red")
lines(1:6,LossPlot[3,],col="green")
lines(1:6,LossPlot[4,],col="orange")
lines(1:6,LossPlot[5,])

Evaluation.tableU2<-WorkflowEvaluationTable(Posterior.Probability,Utp = 2)
Evaluation.tableU3<-WorkflowEvaluationTable(Posterior.Probability,Utp = 3)
Evaluation.tableU4<-WorkflowEvaluationTable(Posterior.Probability,Utp = 4)
Evaluation.tableU5<-WorkflowEvaluationTable(Posterior.Probability,Utp = 5)
Evaluation.tableU6<-WorkflowEvaluationTable(Posterior.Probability,Utp = 6)
UtilityPlot<-data.frame(Evaluation.table$Eutility,Evaluation.tableU2$Eutility,Evaluation.tableU3$Eutility,Evaluation.tableU4$Eutility,Evaluation.tableU5$Eutility,Evaluation.tableU6$Eutility)
#names(UtilityPlot)<-c("LFP1","LFP2","LFP3","LFP4","LFP5","LFP6")
plot(1:6,UtilityPlot[1,],ylim = c(0,100),type = "l",col="blue",main="UTP vs Total Expected Utility",xlab="UTP",ylab="TEU")
lines(1:6,UtilityPlot[2,],col="red")
lines(1:6,UtilityPlot[3,],col="green")
lines(1:6,UtilityPlot[4,],col="orange")
lines(1:6,UtilityPlot[5,])

ExpectedUtilityPlot(Evaluation.table)
```

```{r}
data(BRCATHRESHOLD)
EvaluationExperimentSet<-BRCATHRESHOLD
ReferenceSet<-BRCARPPADATAFINAL
Workflow.Path.Data<-WorkflowPathData(EvaluationExperimentSet,ReferenceSet)
Evaluation.Structure<-BuildEvaluationStructure(Workflow.Path.Data,EvaluationTag=c("Allfeatures","Over1000","Over5000","Over10000"))
Path.Model.Quality<-WorkflowPathModelQuality(Evaluation.Structure)
Model.Quality.Values<-ModelQualityPairs(Path.Model.Quality,method="pearson")
Posterior.Probability<-EstimatePosteriorProbability(Path.Model.Quality,Model.Quality.Values)
Evaluation.table<-WorkflowEvaluationTable(Posterior.Probability)
Evaluation.table

Evaluation.tableL2<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 2)
Evaluation.tableL3<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 3)
Evaluation.tableL4<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 4)
Evaluation.tableL5<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 5)
Evaluation.tableL6<-WorkflowEvaluationTable(Posterior.Probability,Lfp = 6)
LossPlot<-data.frame(Evaluation.table$Eutility,Evaluation.tableL2$Eutility,Evaluation.tableL3$Eutility,Evaluation.tableL4$Eutility,Evaluation.tableL5$Eutility,Evaluation.tableL6$Eutility)
#names(UtilityPlot)<-c("LFP1","LFP2","LFP3","LFP4","LFP5","LFP6")
plot(1:6,LossPlot[1,],ylim = c(0,50),type = "l",col="blue",main="LFP vs Total Expected Utility",xlab="LFP",ylab="TEU")
lines(1:6,LossPlot[2,],col="red")
lines(1:6,LossPlot[3,],col="green")
lines(1:6,LossPlot[4,],col="orange")
lines(1:6,LossPlot[5,])

Evaluation.tableU2<-WorkflowEvaluationTable(Posterior.Probability,Utp = 2)
Evaluation.tableU3<-WorkflowEvaluationTable(Posterior.Probability,Utp = 3)
Evaluation.tableU4<-WorkflowEvaluationTable(Posterior.Probability,Utp = 4)
Evaluation.tableU5<-WorkflowEvaluationTable(Posterior.Probability,Utp = 5)
Evaluation.tableU6<-WorkflowEvaluationTable(Posterior.Probability,Utp = 6)
UtilityPlot<-data.frame(Evaluation.table$Eutility,Evaluation.tableU2$Eutility,Evaluation.tableU3$Eutility,Evaluation.tableU4$Eutility,Evaluation.tableU5$Eutility,Evaluation.tableU6$Eutility)
#names(UtilityPlot)<-c("LFP1","LFP2","LFP3","LFP4","LFP5","LFP6")
plot(1:6,UtilityPlot[1,],ylim = c(0,100),type = "l",col="blue",main="UTP vs Total Expected Utility",xlab="UTP",ylab="TEU")
lines(1:6,UtilityPlot[2,],col="red")
lines(1:6,UtilityPlot[3,],col="green")
lines(1:6,UtilityPlot[4,],col="orange")
lines(1:6,UtilityPlot[5,])


ExpectedUtilityPlot(Evaluation.table)
```



